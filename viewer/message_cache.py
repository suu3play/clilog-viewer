#!/usr/bin/env python3
"""
SQLiteãƒ™ãƒ¼ã‚¹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
é«˜é€ŸåŒ–ã®ãŸã‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥ã¨æ›´æ–°æ¤œçŸ¥ã‚’ä½¿ç”¨
"""
import sqlite3
import hashlib
import json
import re
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Tuple


class MessageCache:
    """é«˜é€Ÿãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.db_path = self.cache_dir / "message_cache.db"
        self.init_db()
    
    def init_db(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        with sqlite3.connect(self.db_path) as conn:
            conn.executescript('''
                -- ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                CREATE TABLE IF NOT EXISTS files (
                    id INTEGER PRIMARY KEY,
                    file_path TEXT UNIQUE NOT NULL,
                    file_hash TEXT NOT NULL,
                    file_size INTEGER NOT NULL,
                    last_modified INTEGER NOT NULL,
                    parsed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    message_count INTEGER NOT NULL
                );

                -- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿
                CREATE TABLE IF NOT EXISTS messages (
                    id INTEGER PRIMARY KEY,
                    file_id INTEGER REFERENCES files(id),
                    message_index INTEGER NOT NULL,
                    timestamp TEXT NOT NULL,
                    role TEXT NOT NULL,
                    content TEXT NOT NULL,
                    content_type TEXT DEFAULT 'text',
                    tool_name TEXT,
                    UNIQUE(file_id, message_index)
                );

                -- é«˜é€Ÿæ¤œç´¢ç”¨FTSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                CREATE VIRTUAL TABLE IF NOT EXISTS message_search USING fts5(
                    content, 
                    content=messages, 
                    content_rowid=id
                );

                -- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
                CREATE INDEX IF NOT EXISTS idx_files_hash ON files(file_hash);
                CREATE INDEX IF NOT EXISTS idx_messages_file_timestamp ON messages(file_id, timestamp);
            ''')

    def get_file_hash(self, file_path: Path) -> str:
        """é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
        stat = file_path.stat()
        
        hasher = hashlib.sha256()
        hasher.update(str(stat.st_size).encode())
        hasher.update(str(int(stat.st_mtime)).encode())
        
        with open(file_path, 'rb') as f:
            # å…ˆé ­1MB
            chunk = f.read(1024 * 1024)
            hasher.update(chunk)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã„å ´åˆã¯æœ«å°¾1KB
            if stat.st_size > 1024 * 1024:
                f.seek(-1024, 2)
                chunk = f.read(1024)
                hasher.update(chunk)
        
        return hasher.hexdigest()

    def is_cached_and_valid(self, file_path: Path) -> Optional[int]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            file_hash = self.get_file_hash(file_path)
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute(
                    "SELECT id FROM files WHERE file_path = ? AND file_hash = ?",
                    (str(file_path), file_hash)
                )
                result = cursor.fetchone()
                return result[0] if result else None
        except Exception as e:
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def get_cached_messages(self, file_id: int) -> List[Dict]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute('''
                SELECT message_index, timestamp, role, content, content_type, tool_name
                FROM messages 
                WHERE file_id = ? 
                ORDER BY message_index
            ''', (file_id,))
            
            return [dict(row) for row in cursor.fetchall()]

    def save_messages(self, file_path: Path, messages: List[Dict]) -> int:
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        file_hash = self.get_file_hash(file_path)
        stat = file_path.stat()
        
        with sqlite3.connect(self.db_path) as conn:
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ä¿å­˜
            cursor = conn.execute('''
                INSERT OR REPLACE INTO files 
                (file_path, file_hash, file_size, last_modified, message_count)
                VALUES (?, ?, ?, ?, ?)
            ''', (str(file_path), file_hash, stat.st_size, int(stat.st_mtime), len(messages)))
            
            file_id = cursor.lastrowid
            
            # æ—¢å­˜ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤
            conn.execute('DELETE FROM messages WHERE file_id = ?', (file_id,))
            
            # æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿å­˜
            for i, msg in enumerate(messages):
                conn.execute('''
                    INSERT INTO messages 
                    (file_id, message_index, timestamp, role, content, content_type, tool_name)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    file_id, i, msg['timestamp'], msg['role'], 
                    msg['content'], msg.get('content_type', 'text'), 
                    msg.get('tool_name')
                ))
            
            # FTS5ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°
            conn.execute('INSERT INTO message_search(message_search) VALUES("rebuild")')
            
            return file_id

    def search_messages(self, query: str, file_ids: List[int] = None, limit: int = 100) -> List[Dict]:
        """FTS5ã«ã‚ˆã‚‹é«˜é€Ÿå…¨æ–‡æ¤œç´¢"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            
            if file_ids:
                placeholders = ','.join('?' * len(file_ids))
                cursor = conn.execute(f'''
                    SELECT m.*, f.file_path
                    FROM message_search s
                    JOIN messages m ON m.id = s.rowid
                    JOIN files f ON f.id = m.file_id
                    WHERE message_search MATCH ? AND m.file_id IN ({placeholders})
                    ORDER BY rank
                    LIMIT ?
                ''', [query] + file_ids + [limit])
            else:
                cursor = conn.execute('''
                    SELECT m.*, f.file_path
                    FROM message_search s
                    JOIN messages m ON m.id = s.rowid  
                    JOIN files f ON f.id = m.file_id
                    WHERE message_search MATCH ?
                    ORDER BY rank
                    LIMIT ?
                ''', (query, limit))
            
            return [dict(row) for row in cursor.fetchall()]

    def get_cached_files(self) -> List[Dict]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute('''
                SELECT id, file_path, file_size, last_modified, parsed_at, message_count
                FROM files 
                ORDER BY last_modified DESC
            ''')
            
            return [dict(row) for row in cursor.fetchall()]

    def clear_cache(self, file_path: Path = None):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        with sqlite3.connect(self.db_path) as conn:
            if file_path:
                conn.execute('DELETE FROM files WHERE file_path = ?', (str(file_path),))
            else:
                conn.execute('DELETE FROM files')
                conn.execute('DELETE FROM messages')


class MarkdownParser:
    """Markdownãƒ•ã‚¡ã‚¤ãƒ«è§£æï¼ˆæ—¢å­˜log_converter.pyã‹ã‚‰ç§»æ¤ãƒ»æœ€é©åŒ–ï¼‰"""
    
    @staticmethod
    def parse_markdown_file(file_path: Path) -> List[Dict]:
        """Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
        messages = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²ï¼ˆ## ã§å§‹ã¾ã‚‹è¡Œï¼‰
        sections = re.split(r'\n## ', content)
        
        for i, section in enumerate(sections):
            if i == 0:  # ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                continue
                
            # ãƒ­ãƒ¼ãƒ«ã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡º
            lines = section.split('\n')
            header = lines[0]
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°: "ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼ (2024-03-31 14:28:15)"
            match = re.match(r'([ğŸ‘¤ğŸ¤–âš™ï¸ğŸ“‹])\s*(\w+).*?\(([^)]+)\)', header)
            if not match:
                continue
                
            emoji, role_name, timestamp = match.groups()
            
            # ãƒ­ãƒ¼ãƒ«æ­£è¦åŒ–
            if emoji == 'ğŸ‘¤':
                role = 'user'
            elif emoji == 'ğŸ¤–':
                role = 'assistant'
            elif emoji == 'ğŸ“‹':
                role = 'summary'
            else:
                role = role_name.lower()
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
            content_lines = lines[1:]
            content = '\n'.join(content_lines).strip()
            
            # "---" åŒºåˆ‡ã‚Šã‚’å‰Šé™¤
            if content.endswith('\n---'):
                content = content[:-4].strip()
            
            # ãƒ„ãƒ¼ãƒ«ä½¿ç”¨æ¤œçŸ¥
            tool_name = None
            content_type = 'text'
            
            tool_match = re.search(r'\[ãƒ„ãƒ¼ãƒ«ä½¿ç”¨:\s*([^\]]+)\]', content)
            if tool_match:
                tool_name = tool_match.group(1)
                content_type = 'tool_use'
            
            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯æ¤œçŸ¥
            if '```' in content:
                content_type = 'code_block'
            
            messages.append({
                'timestamp': timestamp,
                'role': role,
                'content': content,
                'content_type': content_type,
                'tool_name': tool_name
            })
        
        return messages


def load_chat_messages(file_path: Path) -> List[Dict]:
    """çµ±åˆå‡¦ç†ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª â†’ ãƒ‘ãƒ¼ã‚¹ â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜"""
    cache = MessageCache()
    
    # Step 1: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
    file_id = cache.is_cached_and_valid(file_path)
    if file_id:
        print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {file_path.name}")
        return cache.get_cached_messages(file_id)
    
    # Step 2: åˆå›ãƒ‘ãƒ¼ã‚¹
    print(f"åˆå›ãƒ‘ãƒ¼ã‚¹ä¸­: {file_path.name}")
    messages = MarkdownParser.parse_markdown_file(file_path)
    
    # Step 3: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
    cache.save_messages(file_path, messages)
    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†: {len(messages)}ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    
    return messages


def build_initial_cache():
    """å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½œæˆ"""
    cache = MessageCache()
    md_files = list(Path().glob("log_*.md"))
    
    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½œæˆé–‹å§‹: {len(md_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    for file_path in md_files:
        if not cache.is_cached_and_valid(file_path):
            print(f"å‡¦ç†ä¸­: {file_path.name}")
            messages = MarkdownParser.parse_markdown_file(file_path)
            cache.save_messages(file_path, messages)
    
    print("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½œæˆå®Œäº†")


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--build-cache":
            build_initial_cache()
        else:
            file_path = Path(sys.argv[1])
            if file_path.exists():
                messages = load_chat_messages(file_path)
                print(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(messages)}ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
            else:
                print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
    else:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python message_cache.py <markdown_file>")
        print("  python message_cache.py --build-cache")