#!/usr/bin/env python3
"""
ä¼šè©±ãƒ­ã‚°ã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²ã—ã€JSONå½¢å¼ã§å‡ºåŠ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import json
import re
import sqlite3
import hashlib
from datetime import datetime, timezone, timedelta
from pathlib import Path
import configparser
import os
import getpass


def format_timestamp(timestamp_str):
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’JSTå½¢å¼ã«å¤‰æ›"""
    try:
        # UTCæ™‚åˆ»ã‚’ãƒ‘ãƒ¼ã‚¹
        dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
        
        # JSTã«å¤‰æ›ï¼ˆUTC+9ï¼‰
        jst = timezone(timedelta(hours=9))
        dt_jst = dt.astimezone(jst)
        
        return dt_jst.strftime('%Y-%m-%d %H:%M:%S JST')
    except:
        return timestamp_str


def parse_message_date(timestamp_str):
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ—¥ä»˜ã‚’è§£æã—ã¦datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™"""
    try:
        # UTCæ™‚åˆ»ã‚’ãƒ‘ãƒ¼ã‚¹
        dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
        return dt
    except:
        return None


class LogDatabase:
    """ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    def __init__(self, db_path='log_data.db'):
        self.db_path = Path(db_path)
        self.init_database()
    
    def init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒã‚’åˆæœŸåŒ–"""
        with sqlite3.connect(self.db_path) as conn:
            conn.executescript('''
                -- èª­ã¿å–ã‚Šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«
                CREATE TABLE IF NOT EXISTS log_files (
                    id INTEGER PRIMARY KEY,
                    filename TEXT UNIQUE NOT NULL,
                    file_path TEXT NOT NULL,
                    last_modified INTEGER NOT NULL,
                    file_hash TEXT NOT NULL,
                    processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );

                -- ä¼šè©±ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
                CREATE TABLE IF NOT EXISTS conversations (
                    id INTEGER PRIMARY KEY,
                    log_file_id INTEGER REFERENCES log_files(id),
                    role TEXT NOT NULL CHECK(role IN ('user', 'assistant')),
                    timestamp TEXT NOT NULL,
                    content TEXT NOT NULL,
                    filename TEXT NOT NULL
                );

                -- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
                CREATE INDEX IF NOT EXISTS idx_conversations_timestamp ON conversations(timestamp);
                CREATE INDEX IF NOT EXISTS idx_conversations_role ON conversations(role);
                CREATE INDEX IF NOT EXISTS idx_conversations_filename ON conversations(filename);
                CREATE INDEX IF NOT EXISTS idx_log_files_modified ON log_files(last_modified);
            ''')
    
    def get_file_hash(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
        hasher = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    def is_file_changed(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰æ›´ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        file_stat = file_path.stat()
        current_hash = self.get_file_hash(file_path)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                SELECT file_hash, last_modified FROM log_files 
                WHERE file_path = ?
            ''', (str(file_path),))
            
            result = cursor.fetchone()
            if not result:
                return True  # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«
            
            stored_hash, stored_mtime = result
            return current_hash != stored_hash or int(file_stat.st_mtime) != stored_mtime
    
    def register_file(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²"""
        file_stat = file_path.stat()
        file_hash = self.get_file_hash(file_path)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                INSERT OR REPLACE INTO log_files 
                (filename, file_path, last_modified, file_hash)
                VALUES (?, ?, ?, ?)
            ''', (file_path.name, str(file_path), int(file_stat.st_mtime), file_hash))
            
            return cursor.lastrowid
    
    def clear_conversations_for_file(self, log_file_id):
        """ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('DELETE FROM conversations WHERE log_file_id = ?', (log_file_id,))
    
    def insert_conversation(self, log_file_id, role, timestamp, content, filename):
        """ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ç™»éŒ²"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                INSERT INTO conversations 
                (log_file_id, role, timestamp, content, filename)
                VALUES (?, ?, ?, ?, ?)
            ''', (log_file_id, role, timestamp, content, filename))
    
    def get_conversations_in_range(self, start_date=None, end_date=None):
        """æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ç¯„å›²ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            
            if start_date and end_date:
                cursor = conn.execute('''
                    SELECT role, timestamp, content, filename
                    FROM conversations 
                    WHERE datetime(timestamp) BETWEEN datetime(?) AND datetime(?)
                    ORDER BY datetime(timestamp)
                ''', (start_date, end_date))
            elif start_date:
                cursor = conn.execute('''
                    SELECT role, timestamp, content, filename
                    FROM conversations 
                    WHERE datetime(timestamp) >= datetime(?)
                    ORDER BY datetime(timestamp)
                ''', (start_date,))
            elif end_date:
                cursor = conn.execute('''
                    SELECT role, timestamp, content, filename
                    FROM conversations 
                    WHERE datetime(timestamp) <= datetime(?)
                    ORDER BY datetime(timestamp)
                ''', (end_date,))
            else:
                cursor = conn.execute('''
                    SELECT role, timestamp, content, filename
                    FROM conversations 
                    ORDER BY datetime(timestamp)
                ''')
            
            return [dict(row) for row in cursor.fetchall()]


def extract_content(message):
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
    if isinstance(message, dict):
        if 'content' in message:
            content = message['content']
            if isinstance(content, list):
                text_parts = []
                for item in content:
                    if isinstance(item, dict) and item.get('type') == 'text':
                        text_parts.append(item.get('text', ''))
                    elif isinstance(item, dict) and item.get('type') == 'tool_use':
                        tool_name = item.get('name', 'unknown')
                        tool_input = item.get('input', {})
                        text_parts.append(f"[ãƒ„ãƒ¼ãƒ«ä½¿ç”¨: {tool_name}]\n```json\n{json.dumps(tool_input, ensure_ascii=False, indent=2)}\n```")
                return '\n'.join(text_parts)
            elif isinstance(content, str):
                return content
        elif 'role' in message:
            return extract_content(message)
    return str(message)


def clean_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    if not text:
        return ""
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å‡¦ç†
    text = re.sub(r'<command-message>.*?</command-message>', '', text, flags=re.DOTALL)
    text = re.sub(r'<command-name>.*?</command-name>', '', text, flags=re.DOTALL)
    text = re.sub(r'<command-args>.*?</command-args>', '', text, flags=re.DOTALL)
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªãƒã‚¤ãƒ³ãƒ€ãƒ¼ã®å‡¦ç†
    text = re.sub(r'<system-reminder>.*?</system-reminder>', '', text, flags=re.DOTALL)
    
    # ç©ºè¡Œã®æ•´ç†
    text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
    text = text.strip()
    
    return text


def process_log_line(line):
    """ãƒ­ã‚°ã®1è¡Œã‚’å‡¦ç†ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼/ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’è¿”ã™"""
    try:
        data = json.loads(line.strip())
        
        # åŸºæœ¬æƒ…å ±ã®æŠ½å‡º
        timestamp = data.get('timestamp', '')
        user_type = data.get('userType', data.get('type', ''))
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹ã®æŠ½å‡º
        message_data = data.get('message', {})
        role = message_data.get('role', user_type)
        content = extract_content(message_data)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¾ãŸã¯ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿å‡¦ç†
        if role in ['user', 'assistant'] and content:
            content = clean_text(content)
            if content:  # ç©ºã§ãªã„å ´åˆã®ã¿è¿”ã™
                return {
                    'timestamp': timestamp,  # ISOå½¢å¼ã®ã¾ã¾ä¿å­˜
                    'role': role,
                    'content': content
                }
    
    except json.JSONDecodeError:
        # JSONä»¥å¤–ã®è¡Œã¯ç„¡è¦–
        pass
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"å•é¡Œã®ã‚ã‚‹è¡Œ: {line[:100]}...")
    
    return None


def generate_output_filename(input_file, output_directory, username=None):
    """å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å_æ—¥ä»˜å½¢å¼ï¼‰"""
    # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ™‚åˆ»ã‚’å–å¾—ï¼ˆUTCï¼‰
    mod_time_utc = datetime.fromtimestamp(input_file.stat().st_mtime, tz=timezone.utc)
    
    # JSTã«å¤‰æ›
    jst = timezone(timedelta(hours=9))
    mod_time_jst = mod_time_utc.astimezone(jst)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å–å¾—ï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ç«¯æœ«ãƒ¦ãƒ¼ã‚¶ãƒ¼åï¼‰
    if username is None:
        try:
            username = getpass.getuser()
        except Exception:
            username = "unknown"
    
    timestamp = mod_time_jst.strftime('%Y%m%d%H%M%S')
    filename = f"log_{username}_{timestamp}_{input_file.stem}.md"
    return output_directory / filename


def load_processed_files_info(info_file):
    """å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’èª­ã¿è¾¼ã¿"""
    if not info_file.exists():
        return {}
    
    try:
        with open(info_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (json.JSONDecodeError, FileNotFoundError):
        return {}


def save_processed_files_info(info_file, info):
    """å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ä¿å­˜"""
    with open(info_file, 'w', encoding='utf-8') as f:
        json.dump(info, f, indent=2)


def should_process_file(input_file, processed_info):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã¹ãã‹ãƒã‚§ãƒƒã‚¯"""
    file_key = input_file.name
    current_mtime = input_file.stat().st_mtime
    
    if file_key in processed_info:
        last_mtime = processed_info[file_key].get('mtime', 0)
        if current_mtime <= last_mtime:
            return False
    
    return True


def process_log_file_to_database(file_path, database):
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²"""
    messages = []
    filename = file_path.name
    
    print(f"  â†’ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            total_lines = len(lines)
            print(f"  â†’ ç·è¡Œæ•°: {total_lines}è¡Œ")
            
            for i, line in enumerate(lines, 1):
                if line.strip():
                    processed = process_log_line(line)
                    if processed:
                        messages.append(processed)
                
                # 100è¡Œã”ã¨ã«é€²æ—è¡¨ç¤º
                if i % 100 == 0 or i == total_lines:
                    found_messages = len(messages)
                    print(f"    é€²æ—: {i}/{total_lines}è¡Œ ({i/total_lines*100:.1f}%) - ä¼šè©±ãƒ‡ãƒ¼ã‚¿: {found_messages}ä»¶")
                    
    except FileNotFoundError:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        return False
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    if not messages:
        print(f"  â†’ ä¼šè©±ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return True  # ã‚¨ãƒ©ãƒ¼ã§ã¯ãªã„
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²
    try:
        print(f"  â†’ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²ä¸­ ({len(messages)}ä»¶)")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™»éŒ²ã—ã¦IDã‚’å–å¾—
        log_file_id = database.register_file(file_path)
        
        # æ—¢å­˜ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
        database.clear_conversations_for_file(log_file_id)
        
        # ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’100ä»¶ãšã¤ç™»éŒ²ï¼ˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã§é«˜é€ŸåŒ–ï¼‰
        import sqlite3
        with sqlite3.connect(database.db_path) as conn:
            batch_size = 100
            for i in range(0, len(messages), batch_size):
                batch = messages[i:i + batch_size]
                
                # ãƒãƒƒãƒã§INSERT
                conn.executemany('''
                    INSERT INTO conversations 
                    (log_file_id, role, timestamp, content, filename)
                    VALUES (?, ?, ?, ?, ?)
                ''', [(log_file_id, msg['role'], msg['timestamp'], msg['content'], filename) 
                      for msg in batch])
                
                # é€²æ—è¡¨ç¤º
                processed_count = min(i + batch_size, len(messages))
                print(f"    ç™»éŒ²é€²æ—: {processed_count}/{len(messages)}ä»¶ ({processed_count/len(messages)*100:.1f}%)")
        
        print(f"  âœ“ ç™»éŒ²å®Œäº†: {len(messages)}ä»¶ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿")
        return True
        
    except Exception as e:
        print(f"  â†’ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def export_conversations_to_json(database, output_file, start_date=None, end_date=None):
    """ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§å‡ºåŠ›"""
    try:
        conversations = database.get_conversations_in_range(start_date, end_date)
        
        # JSONå½¢å¼ã§å‡ºåŠ›
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(conversations, f, ensure_ascii=False, indent=2)
        
        print(f"JSONå‡ºåŠ›å®Œäº†: {output_file}")
        print(f"å‡ºåŠ›ã—ãŸä¼šè©±ãƒ‡ãƒ¼ã‚¿: {len(conversations)}ä»¶")
        return True
        
    except Exception as e:
        print(f"JSONå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")
        return False


class Config:
    """è¨­å®šç®¡ç†ã‚¯ãƒ©ã‚¹"""
    def __init__(self, config_file='log_converter_config.ini'):
        self.config_file = Path(config_file)
        self.config = configparser.ConfigParser()
        self.load_config()
    
    def load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if self.config_file.exists():
            self.config.read(self.config_file, encoding='utf-8')
        else:
            self.create_default_config()
    
    def create_default_config(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        self.config['DEFAULT'] = {
            'log_directory': '',  # ç©ºã®å ´åˆã¯è‡ªå‹•æ¤œç´¢
            'output_directory': '',  # ç©ºã®å ´åˆã¯ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            'username': '',  # ç©ºã®å ´åˆã¯ç«¯æœ«ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            'max_files': '10',
            'skip_unchanged': 'true',
            'date_start': '',  # é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDï¼‰
            'date_end': ''     # çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDï¼‰
        }
        self.save_config()
        print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {self.config_file}")
    
    def save_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            self.config.write(f)
    
    def get_log_directory(self):
        """ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—"""
        log_dir = self.config.get('DEFAULT', 'log_directory', fallback='')
        if log_dir:
            return Path(log_dir)
        return Path.home() / '.claude' / 'projects'
    
    def get_output_directory(self):
        """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—"""
        output_dir = self.config.get('DEFAULT', 'output_directory', fallback='')
        if output_dir:
            return Path(output_dir)
        return Path.cwd()
    
    def get_max_files(self):
        """æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’å–å¾—"""
        return self.config.getint('DEFAULT', 'max_files', fallback=10)
    
    def get_skip_unchanged(self):
        """æœªå¤‰æ›´ã‚¹ã‚­ãƒƒãƒ—è¨­å®šã‚’å–å¾—"""
        return self.config.getboolean('DEFAULT', 'skip_unchanged', fallback=True)
    
    def get_username(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼åè¨­å®šã‚’å–å¾—"""
        username = self.config.get('DEFAULT', 'username', fallback='')
        if username:
            return username
        try:
            return getpass.getuser()
        except Exception:
            return "unknown"
    
    def get_date_filter(self):
        """æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šã‚’å–å¾—"""
        start_date = self.config.get('DEFAULT', 'date_start', fallback='')
        end_date = self.config.get('DEFAULT', 'date_end', fallback='')
        
        if start_date or end_date:
            return DateFilter(start_date or None, end_date or None)
        return None
    
    def set_date_range(self, start_date, end_date):
        """æ—¥ä»˜ç¯„å›²ã‚’è¨­å®š"""
        self.config.set('DEFAULT', 'date_start', start_date or '')
        self.config.set('DEFAULT', 'date_end', end_date or '')
        self.save_config()


def find_log_files(log_directory=None, start_date=None, end_date=None):
    """Claudeãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé…ä¸‹ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ—¥ã§çµã‚Šè¾¼ã¿ï¼‰"""
    if log_directory is None:
        log_directory = Path.home() / '.claude' / 'projects'
    
    if not log_directory.exists():
        print(f"ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {log_directory}")
        return []
    
    jsonl_files = []
    for root, dirs, files in os.walk(log_directory):
        for file in files:
            if file.endswith('.jsonl'):
                full_path = Path(root) / file
                file_stat = full_path.stat()
                file_modified = datetime.fromtimestamp(file_stat.st_mtime)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ—¥ã«ã‚ˆã‚‹çµã‚Šè¾¼ã¿
                if start_date and file_modified < start_date:
                    continue
                if end_date and file_modified > end_date:
                    continue
                    
                jsonl_files.append(full_path)
    
    # æ›´æ–°æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
    jsonl_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    return jsonl_files


def get_default_date_range():
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ—¥ä»˜ç¯„å›²ã‚’å–å¾—ï¼ˆç›´è¿‘1ãƒ¶æœˆå‰å¾Œã€é–‹å§‹ã¯01æ—¥ï¼‰"""
    now = datetime.now()
    
    # ä»Šæœˆã®1æ—¥
    current_month_start = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    
    # å…ˆæœˆã®1æ—¥
    if now.month == 1:
        last_month_start = current_month_start.replace(year=now.year-1, month=12)
    else:
        last_month_start = current_month_start.replace(month=now.month-1)
    
    # æ¥æœˆã®1æ—¥
    if now.month == 12:
        next_month_start = current_month_start.replace(year=now.year+1, month=1)
    else:
        next_month_start = current_month_start.replace(month=now.month+1)
    
    return last_month_start, next_month_start


def select_log_file(files):
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ"""
    if not files:
        print("JSONLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None
    
    if len(files) == 1:
        print(f"è¦‹ã¤ã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«: {files[0]}")
        return files[0]
    
    print("è¤‡æ•°ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ:")
    for i, file in enumerate(files, 1):
        rel_path = file.relative_to(Path.home() / '.claude' / 'projects')
        file_size = file.stat().st_size
        mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')
        print(f"{i:2d}: {rel_path} ({file_size:,} bytes, æ›´æ–°: {mod_time})")
    
    while True:
        try:
            choice = input("\né¸æŠã—ã¦ãã ã•ã„ (ç•ªå·): ").strip()
            if not choice:
                return None
            index = int(choice) - 1
            if 0 <= index < len(files):
                return files[index]
            else:
                print("ç„¡åŠ¹ãªç•ªå·ã§ã™ã€‚")
        except ValueError:
            print("æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        except KeyboardInterrupt:
            print("\nä¸­æ–­ã—ã¾ã—ãŸã€‚")
            return None


def process_multiple_files_to_database(files, database):
    """è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¸€æ‹¬å‡¦ç†"""
    processed_count = 0
    skipped_count = 0
    total_files = len(files)
    
    print(f"å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {total_files}ä»¶")
    print("=" * 60)
    
    for i, file in enumerate(files, 1):
        print(f"\n[{i}/{total_files}] å‡¦ç†ä¸­: {file.name}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰æ›´ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if not database.is_file_changed(file):
            print(f"  â†’ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœªå¤‰æ›´ï¼‰")
            skipped_count += 1
            continue
        
        success = process_log_file_to_database(file, database)
        if success:
            processed_count += 1
        else:
            print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {file.name} ã®å‡¦ç†ã«å¤±æ•—")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†ã®é€²æ—è¡¨ç¤º
        progress = i / total_files * 100
        print(f"  ğŸ“Š å…¨ä½“é€²æ—: {i}/{total_files}ãƒ•ã‚¡ã‚¤ãƒ« ({progress:.1f}%)")
    
    print("\n" + "=" * 60)
    print(f"ğŸ‰ å‡¦ç†å®Œäº†: {processed_count}ä»¶å‡¦ç†, {skipped_count}ä»¶ã‚¹ã‚­ãƒƒãƒ—")
    return processed_count > 0


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¼šè©±ãƒ­ã‚°ã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²ãƒ»JSONå‡ºåŠ›')
    parser.add_argument('--output', '-o', help='JSONå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: conversations.jsonï¼‰')
    parser.add_argument('--list', action='store_true', help='ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€è¦§è¡¨ç¤º')
    parser.add_argument('--config', help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--force', action='store_true', help='å…¨ãƒ•ã‚¡ã‚¤ãƒ«å¼·åˆ¶å†å‡¦ç†')
    parser.add_argument('--start-date', help='é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ—¥åŸºæº–ï¼‰')
    parser.add_argument('--end-date', help='çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ—¥åŸºæº–ï¼‰')
    parser.add_argument('--json-start-date', help='JSONå‡ºåŠ›ã®é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€ä¼šè©±æ—¥æ™‚åŸºæº–ï¼‰')
    parser.add_argument('--json-end-date', help='JSONå‡ºåŠ›ã®çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€ä¼šè©±æ—¥æ™‚åŸºæº–ï¼‰')
    parser.add_argument('--db-path', help='ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: log_data.dbï¼‰')
    
    args = parser.parse_args()
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = Config(args.config or 'log_converter_config.ini')
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
    database = LogDatabase(args.db_path or 'log_data.db')
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…å®¹ä¸€è¦§è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰
    if args.list:
        conversations = database.get_conversations_in_range()
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿: {len(conversations)}ä»¶")
        
        if conversations:
            print("\næœ€æ–°10ä»¶:")
            for conv in conversations[-10:]:
                timestamp = conv['timestamp'][:19] if len(conv['timestamp']) > 19 else conv['timestamp']
                print(f"  {timestamp} [{conv['role']}] {conv['content'][:50]}...")
        return
    
    # æ—¥ä»˜ç¯„å›²ã®æ±ºå®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ—¥åŸºæº–ï¼‰
    start_date = None
    end_date = None
    
    if args.start_date or args.end_date:
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°æŒ‡å®š
        if args.start_date:
            start_date = datetime.strptime(args.start_date, '%Y-%m-%d')
        if args.end_date:
            end_date = datetime.strptime(args.end_date, '%Y-%m-%d').replace(hour=23, minute=59, second=59)
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ—¥ãƒ•ã‚£ãƒ«ã‚¿: {args.start_date or 'é–‹å§‹æ—¥ãªã—'} ã€œ {args.end_date or 'çµ‚äº†æ—¥ãªã—'}")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç¯„å›²ï¼ˆç›´è¿‘1ãƒ¶æœˆå‰å¾Œï¼‰
        start_date, end_date = get_default_date_range()
        print(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç¯„å›²ã‚’ä½¿ç”¨: {start_date.strftime('%Y-%m-%d')} ã€œ {end_date.strftime('%Y-%m-%d')}")
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢ãƒ»å‡¦ç†
    files = find_log_files(config.get_log_directory(), start_date, end_date)
    if not files:
        print("å¯¾è±¡ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        exit(1)
    
    print(f"è¦‹ã¤ã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«: {len(files)}ä»¶")
    
    # å¼·åˆ¶å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªã‚¢
    if args.force:
        print("å¼·åˆ¶å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™")
        with sqlite3.connect(database.db_path) as conn:
            conn.execute('DELETE FROM conversations')
            conn.execute('DELETE FROM log_files')
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«å‡¦ç†
    success = process_multiple_files_to_database(files, database)
    if not success:
        print("ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        exit(1)
    
    # JSONå‡ºåŠ›
    output_file = args.output or 'conversations.json'
    json_start = args.json_start_date
    json_end = args.json_end_date
    
    if json_start or json_end:
        print(f"JSONå‡ºåŠ›ï¼ˆä¼šè©±æ—¥æ™‚ãƒ•ã‚£ãƒ«ã‚¿ï¼‰: {json_start or 'é–‹å§‹æ—¥ãªã—'} ã€œ {json_end or 'çµ‚äº†æ—¥ãªã—'}")
    
    success = export_conversations_to_json(database, output_file, json_start, json_end)
    if not success:
        exit(1)


if __name__ == "__main__":
    main()